version: '3.8'

x-spark-common: &spark-common # sao âncora que permitem que eu use esse bloco de configuracao mais pra frente -> vou usar em <<: *spark-common
  # image: bitnami/spark:latest # Imagem Docker pro Spark
  # image: bitnami/spark:3.5.6 # Imagem Docker pro Spark -> a image vai ser utilizada do Dockerfile.spark
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs # Mapeia a pasta local './jobs' para '/opt/bitnami/spark/jobs' no contêiner. Vai ser usada pros scripts em spark
  networks:
    - code-with-yu # Conecta os serviços Spark à rede 'code-with-yu'

x-airflow-common: &airflow-common # sao âncora que permitem que eu use esse bloco de configuracao mais pra frente -> vou usar em <<: *spark-common
  build:
    context: . # O contexto de construção (pasta onde o Dockerfile está) é o diretório atual
    dockerfile: Dockerfile # Nome do Dockerfile pra ser usado
  env_file:
    - airflow.env # carrega as variaveis de ambiente do arquivo 'airflow.env'
  volumes:
    - ./jobs:/opt/airflow/jobs # Mapeia a pasta local './jobs' para o Airflow (talvez para DAGs ou recursos de DAG)
    - ./dags:/opt/airflow/dags # Mapeia a pasta local './dags' para o diretório de DAGs do Airflow. ESSENCIAL para o Airflow encontrar suas DAGs.
    - ./logs:/opt/airflow/logs # Mapeia a pasta local './logs' para o diretório de logs do Airflow. Importante para persistir logs.
  depends_on:
    - postgres # Garante que o serviço 'postgres' seja iniciado antes deste. (Ainda sem o healthcheck, então não garante que o Postgres esteja *pronto*)
  networks:
    - code-with-yu # Conecta os serviços Airflow à rede 'code-with-yu'.

x-spark-worker-common: &spark-worker-common
  build:
    context: .
    dockerfile: Dockerfile.spark
  command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 # Comando para iniciar um Spark Worker, conectando-o ao Master
  depends_on:
    - spark-master # garante que o spark-master esteja rodando
  environment: # variaveis de ambiente pro worker
    SPARK_MODE: worker
    SPARK_WORKER_CORES: 2
    SPARK_WORKER_MEMORY: 1g
    SPARK_MASTER_URL: spark://spark-master:7077
    PYSPARK_PYTHON: python3.11
    PYSPARK_DRIVER_PYTHON: python3.11

services: # aqui de fato é cada container que vai ser criado
  spark-master:
    <<: *spark-common # usa a ancora la de cima
    build:
      context: .
      dockerfile: Dockerfile.spark
    command: bin/spark-class org.apache.spark.deploy.master.Master # Comando para iniciar o Spark Master dentro do contêiner
    ports:
      - "9090:8080" # Mapeia a porta 8080 do contêiner (UI do Spark Master) para a porta 9090 no meu host
      - "7077:7077" # Mapeia a porta 7077 do contêiner (comunicação interna do Spark) para a porta 7077 no meu host
    environment:
      PYSPARK_PYTHON: python3.11
      PYSPARK_DRIVER_PYTHON: python3.11
  spark-worker-1:
    <<: [*spark-common, *spark-worker-common] # usa a ancora la de cima
  
  spark-worker-2:
    <<: [*spark-common, *spark-worker-common] # usa a ancora la de cima
  
  postgres:
    image: postgres:14.0
    environment: # variaveis de ambiente pro postgres
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - code-with-yu
    # volumes:
    #   - pgdata:/var/lib/postgresql/data # talvez ver de adicionar um volume nomeado aqui para persistir os dados do banco de dados
    ## esse "pgdata:" trata-se de um volume nomeado que o Docker cria automaticamente - geralmente /var/lib/docker/volumes/ no Linux (WSL)
    # Junto disso, precisa adicionar a seção 'volumes:' no final do arquivo

  # >>> SERVIÇO airflow-init (CRUCIAL!) <<<
  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    # Precisa do 'dockerize' instalado no Dockerfile - comando para evitar que um container inicie antes do outro
    command: >
      bash -cx "
      dockerize -wait tcp://postgres:5432 -timeout 60s;
      airflow db migrate &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin;
      "
    depends_on:
      - postgres
    restart: "no"
    environment:
      _AIRFLOW_DB_UPGRADE: "true"
      _AIRFLOW_WEBSERVER_START: "false"
      _AIRFLOW_SCHEDULER_START: "false"
  
  webserver:
    <<: *airflow-common
    command: airflow webserver # airflow api-server # <--- CORREÇÃO AQUI CASO USE 3.0.3
    ports:
      - "8080:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    # Pendência: Adicionar healthcheck para webserver

  scheduler:
    <<: *airflow-common
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    # Pendência: Adicionar healthcheck para scheduler
  
networks:
  code-with-yu:

# Caso va adicionar volumes pra postgres
# volumes: # Esta seção define os volumes nomeados
#   pgdata: # Declara o volume nomeado 'pgdata'
#     # Você pode adicionar drivers ou opções aqui, mas para o básico, deixar vazio já funciona.